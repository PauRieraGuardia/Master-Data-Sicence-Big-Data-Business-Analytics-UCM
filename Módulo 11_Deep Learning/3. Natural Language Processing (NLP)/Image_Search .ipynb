{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a la Similitud de Imágenes\n",
        "\n",
        "En este notebook, presentaremos la búsqueda de imágenes utilizando `sentence-transformers`, una biblioteca de Python para embeddings de frases, texto e imágenes de última generación.\n",
        "\n",
        "<b>¿Qué es la Similitud de Imágenes?</b>\n",
        "\n",
        "La similitud de imágenes se refiere al proceso de encontrar imágenes que sean visualmente parecidas. Esto puede ir desde encontrar duplicados casi idénticos hasta agrupar imágenes basadas en una semejanza temática. Las implicaciones de esta tecnología son profundas, ya que sustenta sistemas de:\n",
        "\n",
        "* <b>Búsqueda Visual</b>: Los minoristas y mercados en línea utilizan la similitud de imágenes para ofrecer recomendaciones de productos basadas en fotos subidas por el usuario. Esta tecnología mejora la experiencia de compra al permitir a los usuarios buscar productos usando imágenes en lugar de palabras.\n",
        "\n",
        "* <b>Descubrimiento de Contenido</b>: Las plataformas de redes sociales y los sistemas de gestión de contenidos confían en la similitud de imágenes para categorizar y recomendar contenido, ayudando a los usuarios a descubrir nuevas publicaciones relacionadas con lo que ya les gusta.\n",
        "\n",
        "* <b>Archivado Digital</b>: En bibliotecas y archivos, la similitud de imágenes ayuda a organizar, indexar y recuperar contenido visual de vastas bases de datos, facilitando la búsqueda de documentos históricos y obras de arte.\n",
        "\n",
        "* <b>Seguridad y Vigilancia</b>: Los algoritmos de similitud de imágenes pueden identificar objetos o personas de interés en diferentes fotogramas de vídeo o ubicaciones, contribuyendo a los esfuerzos de seguridad y aplicación de la ley.\n",
        "\n",
        "* <b>Atención Sanitaria</b>: En la imagenología médica, las medidas de similitud pueden ayudar a identificar historiales de casos similares, comprender la progresión de enfermedades e incluso asistir en el diagnóstico mediante la comparación de escaneos de pacientes con una base de datos de condiciones conocidas.\n",
        "\n",
        "<b>¿Por qué es un desafío?</b>\n",
        "\n",
        "Las imágenes pueden variar en tamaño, ángulo, iluminación e incluso estar parcialmente ocultas. Los métodos tradicionales que dependen de coincidencias exactas no logran proporcionar resultados relevantes bajo estas condiciones. Por ello, las técnicas modernas de similitud de imágenes emplean el aprendizaje profundo *(deep learning)*, aprovechando particularmente modelos como CLIP *(Contrastive Language-Image Pretraining)* desarrollado por OpenAI, para entender y cuantificar la semejanza de una manera que imita la percepción humana.\n",
        "\n",
        "<b>Objetivos de este Tutorial</b>\n",
        "\n",
        "En este tutorial, profundizaremos en cómo el aprendizaje profundo, especialmente mediante el uso de Sentence Transformers y el modelo CLIP, nos permite mapear imágenes y textos en un espacio vectorial compartido. Este mapeo nos permite realizar tareas de búsqueda y recuperación matizadas, ofreciendo un puente entre las descripciones textuales y el contenido visual.\n",
        "\n",
        "Al final de esta sesión, comprenderás cómo:\n",
        "\n",
        "1. Utilizar el modelo CLIP para crear embeddings de imágenes y texto.\n",
        "\n",
        "2. Implementar un sistema de búsqueda de imágenes que pueda encontrar imágenes similares basadas en consultas de texto.\n",
        "\n",
        "3. Explorar aplicaciones prácticas y consideraciones al desplegar modelos de similitud de imágenes."
      ],
      "metadata": {
        "id": "Lx0nrmhtNcZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda de imágenes\n",
        "\n",
        "En este notebook, presentaremos la búsqueda de imágenes utilizando Sentence Transformers, mapeando imágenes y textos en el mismo espacio vectorial. Esto nos permite realizar tareas de búsqueda y recuperación de imágenes basadas en descripciones textuales.\n",
        "\n",
        "Para lograrlo, utilizaremos el modelo [CLIP (Contrastive Language-Image Pretraining)](https://openai.com/research/clip), el cual está diseñado para aprender un espacio de embedding (incrustación) conjunto tanto para imágenes como para textos.\n",
        "\n",
        "CLIP es un modelo de IA desarrollado por OpenAI. Está diseñado para aprender de una amplia gama de tareas aprovechando la conexión entre el lenguaje natural y las imágenes.\n",
        "\n",
        "* <b>Aprendizaje Multimodal</b>: CLIP es un modelo multimodal que puede entender tanto imágenes como texto. Está preentrenado en un conjunto de datos masivo que contiene pares de imágenes y sus respectivos subtítulos o descripciones de texto, aprendiendo a asociar conceptos visuales con el lenguaje natural.\n",
        "\n",
        "* <b>Aprendizaje Contrastivo (Contrastive Learning)</b>: CLIP aprende optimizando un objetivo contrastivo. Se entrena para reconocer qué pares de imagen-subtítulo son correctos entre un conjunto de ejemplos negativos. Al aprender a puntuar los pares correctos de imagen-texto por encima de los incorrectos, el modelo aprende una representación útil para ambas modalidades.\n",
        "\n",
        "* <b>Arquitectura</b>: CLIP utiliza una arquitectura basada en Transformers para procesar texto y una arquitectura Vision Transformer (ViT) o ResNet para procesar imágenes. Los codificadores (encoders) de imagen y texto se entrenan conjuntamente, lo que permite al modelo alinear ambas modalidades en un espacio de embedding compartido."
      ],
      "metadata": {
        "id": "mNNHHuukOVzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de la libreria sentence-transformers\n",
        "# !pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "mcbq7yWaOwWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerias\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "import pickle\n",
        "import zipfile\n",
        "import copy\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPImage\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "metadata": {
        "id": "3aPCMF-EOzWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we load the respective CLIP model\n",
        "model_name = 'clip-ViT-B-32' # Utilizamos el modelo mencionado\n",
        "model = SentenceTransformer(model_name) # Lo cargamos"
      ],
      "metadata": {
        "id": "qd9BMkYoO7HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "def get_image_from_url(url): # Definimos función para descargar imagenes\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    return img"
      ],
      "metadata": {
        "id": "akVXbrMMO8LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para buscar imágenes, necesitamos un conjunto de imágenes."
      ],
      "metadata": {
        "id": "SrXEURUZPAQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_url_path = 'https://github.com/ezponda/intro_deep_learning/raw/main/images/'\n",
        "img_urls = [\n",
        "    f'{img_url_path}eiffel_tower.jpeg',\n",
        "    f'{img_url_path}taj_mahal.jpeg',\n",
        "    f'{img_url_path}colosseum.jpeg',\n",
        "    f'{img_url_path}great_wall_of_china.jpeg',\n",
        "    f'{img_url_path}statue_of_liberty.jpeg',\n",
        "] # Descargamos varias imagenes de internet, concretamente del git del profesor\n",
        "\n",
        "images = [get_image_from_url(url) for url in img_urls]\n",
        "\n",
        "print('Sample images: ')\n",
        "for url, image in zip(img_urls, images):\n",
        "    print('_'*50)\n",
        "    print(f'url: {url}')\n",
        "    display(image)"
      ],
      "metadata": {
        "id": "e6JzC60qO9O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_embeddings = model.encode(images,\n",
        "                       batch_size=128,\n",
        "                       convert_to_tensor=True,\n",
        "                       show_progress_bar=True) # Procesamos las imagenes y las pasamos a embeddings\n",
        "img_embeddings = img_embeddings.cpu()\n",
        "print(img_embeddings.shape)"
      ],
      "metadata": {
        "id": "eFhisAJPPDSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, definamos una función para realizar la búsqueda de imágenes, dada una consulta y una lista de embeddings de imágenes."
      ],
      "metadata": {
        "id": "S6Y2TWlwPEXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "\n",
        "def image_search(query: str, model: SentenceTransformer, img_embeddings: np.ndarray, # Función para encotnrar las imagenes\n",
        "                 images: List[Image.Image], top_k: int = 2) -> None:\n",
        "    \"\"\"Realiza una búsqueda de imágenes dada una consulta de texto.\n",
        "\n",
        "    Esta función calcula la similitud del coseno entre el embedding de la consulta y los\n",
        "    embeddings de las imágenes, recupera las top_k imágenes con la mayor similitud y las muestra.\n",
        "\n",
        "  Args:\n",
        "        query (str): La consulta de búsqueda como una cadena de texto.\n",
        "        model (SentenceTransformer): El modelo SentenceTransformer utilizado para codificar el texto y las imágenes.\n",
        "        img_embeddings (np.ndarray): Embeddings precalculados de las imágenes.\n",
        "        images (List[Image.Image]): Una lista de objetos de imagen de PIL.\n",
        "        top_k (int): El número de mejores resultados a mostrar\n",
        "\n",
        "    \"\"\" # Ecplicación de la función\n",
        "    query_embedding = model.encode([query])[0]  # Transformarmos la query a un vector\n",
        "\n",
        "    similarities = cosine_similarity([query_embedding], img_embeddings)[0] # computamos las similaridades\n",
        "\n",
        "    top_k_indices = np.argsort(-similarities)[:top_k] # Indices de las imagenes mas parecidas\n",
        "\n",
        "    print(f\"Input query: {query}\\n\") # Printea el input de la query\n",
        "\n",
        "    # Muestra as imagenes y las similaridades en función de la query\n",
        "    for index in top_k_indices:\n",
        "        print('_' * 50)\n",
        "        print(f\"Similarity Score: {similarities[index]:.4f}\")  # Improved readability with formatting\n",
        "        display(images[index])"
      ],
      "metadata": {
        "id": "tgJj-iSJPJJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search('A building in Paris', model, img_embeddings, images, top_k=2) # Probamos el modelo"
      ],
      "metadata": {
        "id": "J2ARUs8oPh05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search('Find me an image of a famous monument in India', model, img_embeddings, images, top_k=2)"
      ],
      "metadata": {
        "id": "Y9L26zdKPiNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search('A building in China', model, img_embeddings, images, top_k=2)"
      ],
      "metadata": {
        "id": "d6Ikx2IYPjbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset de subconjunto de Unsplash\n",
        "\n",
        "[Unsplash](https://unsplash.com/data) es un conjunto de datos de imágenes colaborativo que se comparte de forma abierta.\n",
        "\n",
        "*Vayamos a hacerlo con un dataset mayor*\n"
      ],
      "metadata": {
        "id": "07P6dDl-PkoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el dataset mayor, Unsplash\n",
        "img_folder = './photos/'\n",
        "\n",
        "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
        "    os.makedirs(img_folder, exist_ok=True)\n",
        "\n",
        "    photo_filename = 'unsplash-25k-photos.zip'\n",
        "    if not os.path.exists(photo_filename):\n",
        "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
        "\n",
        "    #\n",
        "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
        "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
        "            zf.extract(member, img_folder)"
      ],
      "metadata": {
        "id": "WNvqMPvcPvGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image_from_path(file_path): # Definicion función para leer la imagen de un url\n",
        "    img = Image.open(file_path) # Abre la imagen de file_path\n",
        "    return img\n",
        "\n",
        "use_precomputed_embeddings = True\n",
        "\n",
        "if use_precomputed_embeddings:\n",
        "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
        "    if not os.path.exists(emb_filename):\n",
        "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
        "\n",
        "    with open(emb_filename, 'rb') as fIn:\n",
        "        img_names, img_embeddings = pickle.load(fIn)\n",
        "\n",
        "\n",
        "    print(\"Images:\", len(img_names))\n",
        "else:\n",
        "    img_names = list(glob.glob('photos/*.jpg'))[:5_000]\n",
        "    print(\"Images:\", len(img_names))\n",
        "    images = [read_image_from_path(img_name) for img_name in  img_names]\n",
        "    img_embeddings = model.encode(images, batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
        "    img_embeddings = img_embeddings.cpu()"
      ],
      "metadata": {
        "id": "1NwINP5NPw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "\n",
        "def image_search_from_path(query, model: SentenceTransformer, img_embeddings: np.ndarray,\n",
        "                           img_folder: str, img_names: List[str], top_k: int = 2) -> None:\n",
        "    \"\"\"Aquí tienes la traducción al español, manteniendo el formato técnico de la documentación:\n",
        "\n",
        "Realiza una búsqueda de imágenes para una consulta textual dada dentro de un conjunto de imágenes ubicadas en una carpeta específica.\n",
        "\n",
        "   Args:\n",
        "        query (str o Image.Image): La consulta textual para buscar imágenes similares.\n",
        "        model (SentenceTransformer): El modelo SentenceTransformer utilizado para la codificación.\n",
        "        img_embeddings (np.ndarray): Los embeddings precalculados de las imágenes.\n",
        "        img_folder (str): La carpeta donde se encuentran almacenadas las imágenes.\n",
        "        img_names (List[str]): Los nombres de archivo de las imágenes.\n",
        "        top_k (int): El número de mejores resultados a devolver.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query_embedding = model.encode([query])[0]\n",
        "        similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
        "        indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
        "        indexes = indexes[np.argsort(-similarities[indexes])]\n",
        "\n",
        "        print(f\"Input query: {query}\\n\")\n",
        "        for index in indexes:\n",
        "            similarity_score = similarities[index]\n",
        "            image_name = img_names[index]\n",
        "            image_path = os.path.join(img_folder, image_name)\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    print('_' * 50)\n",
        "                    print(f\"Similarity: {similarity_score:.4f}\")\n",
        "                    display(copy.deepcopy(img))\n",
        "            except Exception as e:\n",
        "                print(f\"Error displaying image {image_name}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image search: {e}\")\n"
      ],
      "metadata": {
        "id": "r9c1LygIPzeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search_from_path('A building in Paris', model, img_embeddings, img_folder, img_names, top_k=2)"
      ],
      "metadata": {
        "id": "nFKtSMhjQEq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
      ],
      "metadata": {
        "id": "hYKEud5PQFKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search_from_path('Two dogs playing in the snow', model, img_embeddings, img_folder, img_names, top_k=2)"
      ],
      "metadata": {
        "id": "GjcAelDUQHi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search_from_path('Best food in the world', model, img_embeddings, img_folder, img_names, top_k=2)"
      ],
      "metadata": {
        "id": "01Kv7yPmm0io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda de imagen a imagen (Image-to-Image Search)\n",
        "\n",
        "También puedes utilizar este método para realizar búsquedas de imagen a imagen. Para lograrlo, debes pasar `get_image_from_url(url)` al método de búsqueda. Este devolverá entonces imágenes similares."
      ],
      "metadata": {
        "id": "UafsoZirQIq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = get_image_from_url(img_urls[0])\n",
        "img"
      ],
      "metadata": {
        "id": "2oBa5_CaQRJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_search_from_path(img, model, img_embeddings, img_folder, img_names, top_k=5)"
      ],
      "metadata": {
        "id": "5Hlbgr38QShE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
