{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Búsqueda Semántica y Preguntas y Respuestas (QA)\n",
        "\n",
        "En este notebook, presentaremos la búsqueda semántica y los sistemas de preguntas y respuestas (question-answering) utilizando [`sentence-transformers`](https://sbert.net/), una biblioteca de Python para embeddings de frases que esta por encima de [Hugging Face](https://huggingface.co/), texto e imágenes de última generación. Estos embeddings son útiles para tareas de similitud semántica, tales como la recuperación de información y los sistemas de respuesta a preguntas.\n",
        "\n",
        "*Empezamos a aplicar los llamados transformers*"
      ],
      "metadata": {
        "id": "-4d2AgeyQkMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de la libreria sentence-transformers\n",
        "# !pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "zP0QOsxfQo1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Permite manipular archivos JSON\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "# SentenceTransformer: Para convertir frases en vectores numéricos (embeddings).\n",
        "# CrossEncoder: Para re-clasificar resultados comparando pares de frases de forma profunda.\n",
        "# util: Funciones utilitarias para búsqueda semántica y cálculo de similitudes.\n",
        "from sklearn.metrics.pairwise import cosine_similarity # Se usa para calcular qué tan parecidos son dos vectores (mide el ángulo entre ellos).\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time # Permite medir el tiempo de ejecución del código\n",
        "import gzip # Sirve para comprimir o descomprimir archivos sobre la marcha\n",
        "import os # Permite interactuar con el sistema operativo"
      ],
      "metadata": {
        "id": "MhVPYtWwQtio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí tienes la traducción al español:\n",
        "\n",
        "Utilizaremos un modelo de Sentence Transformer pre-entrenado para generar embeddings de frases. Hay muchos modelos pre-entrenados disponibles [aquí](https://www.sbert.net/docs/pretrained_models.html)."
      ],
      "metadata": {
        "id": "9TaHMxriQvQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'all-MiniLM-L6-v2' # Define un string con el nombre específico del modelo preentrenado que queremos descargar de la biblioteca de Hugging Face.\n",
        "model = SentenceTransformer(model_name) # Cargamos el modelo definido de Hugging Face"
      ],
      "metadata": {
        "id": "PQokjQU-RLSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para nuestra tarea de búsqueda semántica y de preguntas y respuestas, necesitamos una lista de documentos o párrafos en los que buscar información relevante."
      ],
      "metadata": {
        "id": "ggn9gUhgRPe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "paragraphs = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
        "    \"The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\",\n",
        "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
        "    \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\",\n",
        "    \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\"\n",
        "] # Definimos varios parrafos\n",
        "\n",
        "paragraphs = np.array(paragraphs) # Los convertimos en un array de la libreria Numpy"
      ],
      "metadata": {
        "id": "sdKbzSaZRNna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = model.encode(paragraphs) # Aplicamos el modelo a los parrafos para que los convierta en vectores\n",
        "print(corpus_embeddings.shape) # Imprimimos la forma de dichos vectores"
      ],
      "metadata": {
        "id": "wuCtdNzoRT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, definamos una función para realizar la búsqueda semántica, dada una consulta y una lista de embeddings de párrafos."
      ],
      "metadata": {
        "id": "Q2hu0kYERWte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos una función para definir la busqueda semantica, el modelo nos devolvera la frase mas cercana a la query\n",
        "def semantic_search(query, model, corpus_embeddings, paragraphs, top_k=2):\n",
        "    query_embedding = model.encode([query])[0] # Convierte la query en un vector\n",
        "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0] # busca la frase similar dentro de los vectores creados anteriormente\n",
        "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
        "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
        "    print(f\"Input query: {query}\") # Printea la query\n",
        "    print()\n",
        "    for text, sim in zip(list(paragraphs[indexes]), similarities[indexes].tolist()):\n",
        "        print(f\"{sim:.3f}\\t{text}\") # Printea la respuesta\n",
        "\n",
        "\n",
        "semantic_search('Where is the Colosseum', model, corpus_embeddings, paragraphs, top_k=2)"
      ],
      "metadata": {
        "id": "MTSSjIPvRYem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos dará la similaridad de la query en los paragraphs previamente definidos"
      ],
      "metadata": {
        "id": "Gv3cDDIlX6pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos multilingües\n",
        "\n",
        "En [Hugging Face](https://huggingface.co/) podemos encontrar más tipo de modelos de Tranfer Learning. Podemos seleccionar el que más nos convenga.  "
      ],
      "metadata": {
        "id": "LzMjSqW6RdX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Este esta usando el previamente definido, el 'all-MiniLM-L6-v2'\n",
        "semantic_search('¿Dónde está el Coliseo?', model, corpus_embeddings, paragraphs, top_k=2)"
      ],
      "metadata": {
        "id": "gUoK2Mp6Rbvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos modelos multilingües disponibles [aquí](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models).\n",
        "\n",
        "Tambien podemos encontrar otros modelos en [HuggingFace](https://huggingface.co/)"
      ],
      "metadata": {
        "id": "ZQFlRdK6Rkn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vayamos a utilizar otro modelo desde la librería HuggingFace\n",
        "model_name = 'BAAI/bge-m3' # Nombramos el modelo escogido\n",
        "multi_model = SentenceTransformer(model_name) # Descargamos el modelo escogido"
      ],
      "metadata": {
        "id": "FiNCcpE-RiF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi_model es un objeto de un modelo pre-entrenado (ej. SentenceTransformer)\n",
        "# El método .encode() transforma la lista de textos 'paragraphs' en vectores numéricos\n",
        "multi_corpus_embeddings = multi_model.encode(paragraphs)\n",
        "# .shape es un atributo de las matrices de NumPy que indica las dimensiones\n",
        "# Se imprime para verificar cuántos párrafos se procesaron y el tamaño del vector de cada uno\n",
        "print(multi_corpus_embeddings.shape)"
      ],
      "metadata": {
        "id": "MZCMtd-HRqzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 vectores de 1024 dimensiones"
      ],
      "metadata": {
        "id": "eBZL5hhy_ur6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\n",
        "    '¿Dónde está el Coliseo?', # 1. La consulta (query) en lenguaje natural.\n",
        "    multi_model, # 2. El modelo que convierte la pregunta en un vector.\n",
        "    multi_corpus_embeddings, # 3. La base de datos de vectores que generamos antes.\n",
        "    paragraphs, # 4. El texto original para poder mostrar la respuesta.\n",
        "    top_k=2 # 5. Indica que solo devuelva los 2 mejores resultados.\n",
        "    )"
      ],
      "metadata": {
        "id": "1OCk23PNRrJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda semántica en Wikipedia\n",
        "\n",
        "Como conjunto de datos, utilizamos la Wikipedia en inglés sencillo (Simple English Wikipedia). En comparación con la versión completa en inglés, esta cuenta con solo unos 170,000 artículos. Hemos dividido estos artículos en párrafos."
      ],
      "metadata": {
        "id": "RvAoB0PfRsbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz' # Definimos la ruta local donde se guardará el archivo descargado\n",
        "\n",
        "if not os.path.exists(wikipedia_filepath):\n",
        "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath) # Comprobamos si el archivo ya existe en nuestra carpeta para no descargarlo dos veces\n",
        "\n",
        "passages = [] # Inicializamos una lista vacía para guardar todos los fragmentos de texto\n",
        "\n",
        "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn: # Abrimos el archivo .gz (comprimido) en modo lectura de texto ('rt') con codificación UTF-8\n",
        "    for line in fIn: # Creamos un bucle para el archivo abierto\n",
        "        data = json.loads(line.strip()) # Convertimos la línea de texto en diccionario de Python\n",
        "        for paragraph in data['paragraphs']: # Para cada unidad en los datos parapgraphs\n",
        "            #\n",
        "            passages.append(data['title']+':  '+ paragraph) # Combinamos titulo con el paragrafo\n",
        "\n",
        "# Mostramos cada elemento\n",
        "print(\"Passages:\", len(passages))\n",
        "print(passages[0])\n",
        "print(passages[1])"
      ],
      "metadata": {
        "id": "-LUM6Q0oR2Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_passages = np.array(passages[:5000]) # Reducimos los datos extraidos, pues tene,os 509.663 y los reducimos a 5000\n",
        "reduced_passages.shape"
      ],
      "metadata": {
        "id": "L5DQlceLR2YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = model.encode(reduced_passages, show_progress_bar=True) # Aplicamos el modelo"
      ],
      "metadata": {
        "id": "7al4dQ9XR3c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search('Best american actor', model, corpus_embeddings, reduced_passages, top_k=2) # Buscamos la frase mas cercana"
      ],
      "metadata": {
        "id": "Gl4Eh0FAR5Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search('Number countries Europe', model, corpus_embeddings, reduced_passages, top_k=2)# Buscamos la frase mas cercana"
      ],
      "metadata": {
        "id": "B5p_VZmGR6x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pregunta 1: Carga un modelo de Sentence Transformer pre-entrenado diferente y compara su rendimiento con el del modelo anterior utilizando el mismo conjunto de párrafos y consultas. ¿Qué modelo funciona mejor?"
      ],
      "metadata": {
        "id": "L9SignYVR8Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos un modelo que nos guste de sentence-transformer, para ello vamos a hugging face a echarle un ojo\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "new_model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "Eri038E-SH9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = new_model.encode(paragraphs,show_progress_bar=True)\n"
      ],
      "metadata": {
        "id": "Qm9yLGu4DbtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search('¿Dónde está el Coliseo?', new_model, corpus_embeddings, paragraphs, top_k=2 )"
      ],
      "metadata": {
        "id": "976pHdFPEPnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pregunta 2: Encontrar duplicados de texto Intenta encontrar textos duplicados o casi duplicados en un corpus determinado basándote en su similitud semántica utilizando sentence-transformers."
      ],
      "metadata": {
        "id": "GJgIwsTESJkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"e un giorno soleggiato\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The quick brown fox leaps over the lazy dog.\",\n",
        "    \"The sky is blue, and the grass is green.\",\n",
        "    \"The grass is green, and the sky is blue.\",\n",
        "    \"It's a sunny day today.\",\n",
        "    \"The weather is sunny today.\",\n",
        "    \"She was wearing a beautiful red dress.\",\n",
        "    \"She had on a gorgeous red dress.\",\n",
        "    \"I'm going to the supermarket to buy some groceries.\",\n",
        "    \"I'm heading to the supermarket to purchase some groceries.\",\n",
        "    \"He didn't like the movie because it was too long.\",\n",
        "    \"He disliked the movie as it was too lengthy.\",\n",
        "    \"The train was delayed due to technical issues.\",\n",
        "    \"Technical issues caused the train to be delayed.\",\n",
        "    \"I'll have a cup of coffee with milk and sugar, please.\",\n",
        "    \"Can I get a coffee with milk and sugar, please?\",\n",
        "    \"The conference was very informative and interesting.\",\n",
        "    \"The conference turned out to be interesting and informative.\",\n",
        "    \"He enjoys listening to classical music in his free time.\",\n",
        "    \"In his leisure time, he likes to listen to classical music.\",\n",
        "    \"Please make sure you turn off the lights before leaving.\",\n",
        "    \"Before leaving, ensure that you switch off the lights.\",\n",
        "    \"The boy was delighted with the gift he received.\",\n",
        "    \"Receiving the present made the young lad ecstatic.\",\n",
        "    \"She has a preference for Italian cuisine.\",\n",
        "    \"Her favorite type of food is from Italy.\",\n",
        "    \"The software engineer resolved the issue by modifying the code.\",\n",
        "    \"By altering the programming, the tech expert fixed the problem.\",\n",
        "    \"Due to the inclement weather, the baseball game was postponed.\",\n",
        "    \"The baseball match was rescheduled because of bad weather conditions.\",\n",
        "    \"The house was engulfed in a raging fire.\",\n",
        "    \"Flames rapidly consumed the residence.\",\n",
        "    \"He is constantly browsing the internet for the latest news.\",\n",
        "    \"He frequently scours the web to stay updated on current events.\",\n",
        "    \"The puppy was playing with a toy in the garden.\",\n",
        "    \"In the yard, the young dog was frolicking with its plaything.\",\n",
        "    \"The artist painted a beautiful landscape on the canvas.\",\n",
        "] # Definimos una nueva base de datos"
      ],
      "metadata": {
        "id": "S7XTVxUxSOS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionamos un nuevo modelo\n",
        "model = 'BAAI/bge-m3'\n",
        "model = SentenceTransformer(model)"
      ],
      "metadata": {
        "id": "r-WYAIc7SPuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obgtenemos los vectores del corpus\n",
        "embeddings = model.encode(corpus,show_progress_bar=True)"
      ],
      "metadata": {
        "id": "6N6kTEjiSQ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold = 0.8  # Definimos el indice de similaridad\n",
        "\n",
        "duplicates = [] # Creamos una lista vacia llamada duplicates\n",
        "\n",
        "for i, emb1 in enumerate(embeddings): # Aplica un bulce en los embeddings enumerados al primer embedding\n",
        "    for j, emb2 in enumerate(embeddings[i + 1:]): # # Aplica un bulce en los embeddings enumerados al embedding +1\n",
        "        similarity = cosine_similarity([emb1], [emb2])[0][0] # Apica la similaridad\n",
        "        if similarity > similarity_threshold: # si la similaridad es mayor al umbral, los guarda en la duplicates\n",
        "            duplicates.append((corpus[i], corpus[i + j + 1], similarity))"
      ],
      "metadata": {
        "id": "U62FjqraSSGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplicate sentences:\")\n",
        "for sent1, sent2, sim in duplicates:\n",
        "    print(f\"{sent1} | {sent2} | Similarity: {sim:.2f}\")\n",
        "    print() # Imprime las frases duplicadas"
      ],
      "metadata": {
        "id": "vINSRTcXSUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agrupación de documentos (Document Clustering)\n",
        "\n",
        "El agrupamiento por K-means es un algoritmo popular de aprendizaje automático no supervisado que agrupa puntos de datos en k clústeres basándose en su similitud. En nuestro caso, queremos agrupar documentos basándonos en su similitud semántica. El algoritmo requiere que especifiquemos de antemano el número de clústeres k."
      ],
      "metadata": {
        "id": "FxDdTAjWSVI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"The apple is a sweet fruit\",\n",
        "    \"Oranges are citrus fruits\",\n",
        "    \"Bananas are rich in potassium\",\n",
        "    \"Strawberries are red fruits\",\n",
        "    \"Dogs are domesticated animals\",\n",
        "    \"Cats are also pets\",\n",
        "    \"Elephants are the largest land mammals\",\n",
        "    \"Cows provide us with milk\",\n",
        "    \"Sharks are marine predators\",\n",
        "    \"Whales are the largest marine mammals\",\n",
        "    \"Dolphins are very intelligent\",\n",
        "    \"Artificial intelligence is the future\",\n",
        "    \"Machine learning is a subset of AI\",\n",
        "    \"Deep learning is a part of machine learning\",\n",
        "    \"Neural networks are used in deep learning\",\n",
        "] #  Nuevo dataset\n",
        "\n",
        "df = pd.DataFrame({'documents': corpus}) # Lo pasa a dataframe"
      ],
      "metadata": {
        "id": "6BAFaiY4ScBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2') # Nuevo modelo aplicado de Hugging Face\n",
        "\n",
        "document_embeddings = model.encode(corpus) # Aplica vectores al corpus"
      ],
      "metadata": {
        "id": "PulPW10kSf6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans # Importamos el algoritmo KMeans\n",
        "num_clusters = 3 # El numero de clusters que queremos\n",
        "clustering_model = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10) # Definimos el algoritmo kmeans\n",
        "clustering_model.fit(document_embeddings) # Lo aplicamos al corpus vectorizado\n",
        "cluster_assignment = clustering_model.labels_  # Definimos los labels\n",
        "\n",
        "df['cluster'] = cluster_assignment # Printeamos los distintos clusters"
      ],
      "metadata": {
        "id": "p1UY9L-dSg96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(df[df['cluster'] == i]['documents'].values, \"\\n\") # Imrpime los distintos clusters"
      ],
      "metadata": {
        "id": "XlbIiRtnShWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detección de Comunidades con Sentence Transformers\n",
        "\n",
        "La librería `sentence_transformers` proporciona una utilidad para la detección de comunidades, la cual aplica un umbral (threshold) sobre la puntuación de similitud de coseno para identificar grupos distintos de oraciones que son semánticamente similares. Este método es especialmente útil para organizar grandes volúmenes de texto (corpus) en grupos significativos.\n",
        "\n",
        "La función está diseñada para encontrar clústeres basados en el significado. Estos son sus componentes principales:\n",
        "\n",
        "* `document_embeddings`: Es la lista de vectores (embeddings) de los documentos de tu corpus. Se pueden crear con cualquier modelo de Sentence Transformer. Deben tener la forma de un tensor 2D o una lista de tensores 1D, donde cada vector representa el significado semántico del documento.\n",
        "\n",
        "* `threshold (Umbral)`: Un valor decimal entre 0 y 1 que determina el punto de corte para unir dos documentos en una misma comunidad. Se basa en la similitud de coseno. Si la similitud entre dos documentos es mayor al umbral, se consideran parte de la misma comunidad.\n",
        "\n",
        "* `min_community_size`: El número mínimo de documentos que debe tener un grupo para ser válido. Si una comunidad tiene menos documentos que este valor, se descarta. El valor por defecto es 1, pero aumentarlo ayuda a filtrar el \"ruido\" y encontrar temas más relevantes.\n",
        "\n",
        "* `batch_size`: Dado que la función calcula similitudes entre pares de documentos, puede consumir mucha memoria en corpus grandes. Los cálculos se realizan en lotes (batches); un tamaño mayor de lote es más rápido pero consume más RAM, mientras que uno menor es más lento pero eficiente en memoria.\n",
        "\n",
        "La función devuelve una lista de comunidades, donde cada comunidad es una lista de índices que corresponden a la posición original de los documentos en tu lista inicial. Cada grupo representa un conjunto de documentos semánticamente similares según el umbral configurado."
      ],
      "metadata": {
        "id": "XPSE6CAxSj-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.util import community_detection\n",
        "document_embeddings = model.encode(\n",
        "    corpus, show_progress_bar=True, convert_to_tensor=True\n",
        ")\n",
        "communities = community_detection(\n",
        "    document_embeddings, threshold=0.5, min_community_size=2, batch_size=1024\n",
        ")\n",
        "for i, comm in enumerate(communities):\n",
        "    print('_'*50)\n",
        "    print(f'community: {i}, size: {len(comm)}')\n",
        "    print('\\n'.join([corpus[ind] for ind in comm]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "n8tGoQKTSrA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la salida, veremos las comunidades de oraciones semánticamente similares. Ten en cuenta que la elección del valor del umbral puede afectar enormemente los resultados: un umbral más bajo dará lugar a comunidades más grandes pero menos cohesionadas, mientras que un umbral más alto dará lugar a comunidades más pequeñas pero más estrechamente vinculadas.\n",
        "\n",
        "La función community_detection es una forma rápida y eficiente de agrupar oraciones similares, pero ten en cuenta que es un método bastante simple basado en la aplicación de un umbral a la similitud de coseno, y métodos de detección de comunidades más sofisticados podrían arrojar mejores resultados para ciertas tareas o conjuntos de datos.\n",
        "\n",
        "Esta función es una excelente manera de explorar la estructura semántica de tu corpus y obtener una comprensión de alto nivel de los principales temas o tópicos en tus datos de texto."
      ],
      "metadata": {
        "id": "jhUJJZAMSro9"
      }
    }
  ]
}