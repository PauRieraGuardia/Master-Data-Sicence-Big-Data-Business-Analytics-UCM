{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b> 1. Dense Neural Network (ANN) model in Keras </b>\n",
        "\n",
        "Una Red Neuronal Densa (ANN – Artificial Neural Network) es el tipo más simple de red neuronal artificial.\n",
        "Está compuesta por capas totalmente conectadas, donde cada neurona de una capa se conecta con todas las neuronas de la siguiente.\n",
        "Se utiliza en problemas de clasificación, regresión o predicción, donde los datos de entrada pueden representarse como vectores de características. (datos estructurados)\n",
        "\n",
        "#### <b>Estructura básica de una red neuronal densa</b>\n",
        "\n",
        "Una arquitectura de red neuronal densa consta de cuatro partes principales:\n",
        "\n",
        "* Capa de entrada (Input layer): recibe los datos iniciales.\n",
        "\n",
        "* Capas ocultas (Hidden layers): realizan transformaciones no lineales.\n",
        "\n",
        "* Capa de salida (Output layer): genera la predicción final."
      ],
      "metadata": {
        "id": "gdxxNwtyQb5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerias necesarias\n",
        "\n",
        "Antes de comenzar a construir una red neuronal, debemos importar las librerías esenciales para definir, entrenar y analizar el modelo.\n",
        "Estas librerías cubren distintos aspectos: modelado, preprocesamiento, visualización y optimización."
      ],
      "metadata": {
        "id": "1iiIeJvBR06l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf                                     # Framework principal\n",
        "from tensorflow.keras import layers, models, optimizers     # Definición y compilación de modelos\n",
        "from tensorflow.keras import callbacks                      # Herramientas de control durante el entrenamiento\n",
        "import numpy as np                                          # Operaciones numéricas y arrays\n",
        "import pandas as pd                                         # Manejo de datasets estructurados (CSV, DataFrames)\n",
        "import matplotlib.pyplot as plt                             # Visualización de métricas y resultados\n",
        "from sklearn.model_selection import train_test_split        # División del dataset en train/test\n",
        "from sklearn.preprocessing import StandardScaler            # Normalización y escalado de variables numéricas\n",
        "import keras_tuner as kt                                    # Búsqueda automática de hiperparámetros"
      ],
      "metadata": {
        "id": "dKfVE9F3R6hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Fase de Construcción\n",
        "La construcción del modelo consiste en definir la arquitectura de la red neuronal, es decir, su estructura interna:\n",
        "qué capas tendrá, cuántas neuronas incluirá y qué funciones de activación aplicará."
      ],
      "metadata": {
        "id": "yY-JHe-idIsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1. Capa de entrada\n",
        "\n",
        "La capa de entrada define la forma de los datos que el modelo recibirá. No realiza cálculos ni activaciones, solo distribuye la información hacia las capas ocultas.\n",
        "\n",
        "con la API Funcional\n",
        "\n",
        "```python\n",
        "inputs = layers.Input(shape=(n_features,), name='input_layer')\n",
        "```\n",
        "\n",
        "Con la API Secuencial:\n",
        "\n",
        "```python\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(n_features,), name='input_layer'))\n",
        "```"
      ],
      "metadata": {
        "id": "ZdPuptmORQbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2. Capas ocultas (Hidden Layers)\n",
        "\n",
        "Las capas ocultas son el núcleo de la red neuronal. Cada neurona combina las entradas mediante una suma ponderada más un sesgo (bias), y aplica una <b>función de activación</b> que introduce no linealidad, permitiendo a la red aprender relaciones complejas.\n",
        "\n",
        "con la API Funcional:\n",
        "\n",
        "```python\n",
        "hidden_1 = layers.Dense(units=128, activation='relu', name='hidden_1')(inputs)\n",
        "hidden_2 = layers.Dense(units=64, activation='relu', name='hidden_2')(hidden_1)\n",
        "```\n",
        "\n",
        "Con la API Secuencial\n",
        "```python\n",
        "model.add(layers.Dense(128, activation='relu', name='hidden_1'))\n",
        "model.add(layers.Dense(64, activation='relu', name='hidden_2'))\n",
        "```\n"
      ],
      "metadata": {
        "id": "Qpj8SEx8THtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones de activación en capas ocultas\n",
        "\n",
        "Las funciones de activación son parte estructural de las capas ocultas. Sin ellas, la red sería equivalente a una regresión lineal.\n",
        "\n",
        "Encontramos las siguientes funciones de activación:\n",
        "\n",
        "``ReLU (Rectified Linear Unit)``\n",
        "- Fórmula: `f(x) = max(0, x)`  \n",
        "- Evita el problema del gradiente desvanecido y acelera el entrenamiento.  \n",
        "- Uso típico: capas ocultas (la más utilizada).\n",
        "- Ejemplo de caso de uso: En un modelo de regresión para predecir precios de viviendas,.\n",
        "\n",
        "Con la API Funcional\n",
        "```python\n",
        "x = layers.Dense(128, activation='relu')(inputs)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "```\n",
        "\n",
        "Con la API Secuencial\n",
        "```python\n",
        "layers.Dense(128, activation='relu', name='hidden_1'),\n",
        "layers.Dense(64, activation='relu', name='hidden_2'),\n",
        "```\n"
      ],
      "metadata": {
        "id": "GBGGIQq3UhQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3. Capa de salida (Output Layer)\n",
        "\n",
        "La capa de salida produce la predicción final del modelo. Su número de neuronas y función de activación depende del tipo de problema que se quiera resolver."
      ],
      "metadata": {
        "id": "s-yNRHkgZ_pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones de activación en capas de salida (Output Layers)"
      ],
      "metadata": {
        "id": "80tdJ5-daNtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <b> Clasificación binaria </b>\n",
        "`Sigmoid`\n",
        "\n",
        "* Devuelve valores entre 0 y 1.\n",
        "\n",
        "* Se utiliza para estimar la probabilidad de pertenecer a una clase.\n",
        "\n",
        "* Se combina con la función de pérdida binary_crossentropy.\n",
        "\n",
        "* Ejemplo: Clasificar correos como spam (1) o no spam (0).\n",
        "\n",
        "con la API Funcional\n",
        "```python\n",
        "output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "````\n",
        "\n",
        "con la API Secuencial\n",
        "```python\n",
        "layers.Dense(1, activation='sigmoid', name='output')\n",
        "````\n",
        "\n",
        "##### <b> Clasificaión multiclase </b>\n",
        "\n",
        "``Softmax ``\n",
        "\n",
        "* Convierte las salidas en probabilidades que suman 1.\n",
        "\n",
        "* Cada neurona representa una clase distinta.\n",
        "\n",
        "* Se combina con la función de pérdida categorical_crossentropy.\n",
        "\n",
        "* Ejemplo: Clasificación de flores en tres categorías (setosa, versicolor, virginica).\n",
        "\n",
        "con la API Funcional\n",
        "```python\n",
        "output = layers.Dense(numero de clases, activation='softmax', name='output')(x)\n",
        "````\n",
        "\n",
        "con la API Secuencial\n",
        "```python\n",
        "layers.Dense(numero de clases, activation='softmax', name='output')\n",
        "````\n",
        "\n",
        "##### <b> Regresión</b>\n",
        "\n",
        "`Tanh` (Tangente hiperbólica) o Linear\n",
        "\n",
        "* Devuelve valores entre -1 y 1, útil si los valores objetivo están normalizados.\n",
        "\n",
        "* Se combina con funciones de pérdida como mse o mae.\n",
        "\n",
        "* Ejemplo: Predicción de un valor continuo previamente normalizado a [-1, 1].\n",
        "\n",
        "con la API Funcional\n",
        "```python\n",
        "output = layers.Dense(1, activation='tanh', name='output')(x)\n",
        "\n",
        "````\n",
        "\n",
        "con la API Secuencial\n",
        "```python\n",
        "layers.Dense(1, activation='tanh', name='output')\n",
        "````\n",
        "`linear`(sin activación)\n",
        "* No aplica ninguna función de activación, por lo que la salida puede tomar cualquier valor real (positivo o negativo).\n",
        "* Es la opción más común en problemas de regresión, donde el modelo debe predecir valores continuos sin límite.\n",
        "* Se combina habitualmente con funciones de pérdida como MSE (mean squared error) o MAE (mean absolute error).\n",
        "* Ejemplo: Predicción del precio de una vivienda o temperatura.\n",
        "\n",
        "con la API Funcional\n",
        "```python\n",
        "output = layers.Dense(1, activation='linear', name='output')(x)\n",
        "````\n",
        "\n",
        "con la API Secuencial\n",
        "```python\n",
        "layers.Dense(1, activation='linear', name='output')\n",
        "````\n"
      ],
      "metadata": {
        "id": "GrlJzz5jaUw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.4. Construcción completa del modelo."
      ],
      "metadata": {
        "id": "y5jbu0ktcmDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API funcional\n",
        "inputs = layers.Input(shape=(10,), name='input')\n",
        "x = layers.Dense(128, activation='relu', name='hidden_1')(inputs)\n",
        "x = layers.Dense(64, activation='relu', name='hidden_2')(x)\n",
        "output = layers.Dense(3, activation='softmax', name='output')(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=output, name='ANN_model')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IR58BU9Gcg3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Secuencial:\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),\n",
        "    layers.Dense(128, activation='relu', name='hidden_1'),\n",
        "    layers.Dense(64, activation='relu', name='hidden_2'),\n",
        "    layers.Dense(3, activation='softmax', name='output')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-CkdQnKWcup6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Fase de compilación\n",
        "\n",
        "Una vez construida la arquitectura de la red neuronal (definidas las capas, sus funciones de activación y conexiones), el siguiente paso es compilar el modelo.\n",
        "\n",
        "Durante la compilación, se especifica cómo el modelo va a aprender. Para ello, se definen tres elementos esenciales:\n",
        "\n",
        "1. La función de péridda (loss): indica qué mide el modelo y qué debe minimizar\n",
        "2. El opimizador (optimizer): indica cómo se actualizan los pesos para reducir la pérdida.\n",
        "3. Las métricas (metrics): sirven para moniotirizar el rendimiento durante el entrenamiento, pero no afectan al aprendizaje."
      ],
      "metadata": {
        "id": "UQaiDkvgeSgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>Función de pérdida (loss)</b>\n",
        "\n",
        "La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. El objetivo del entrenamiento es minizimar esta función.\n",
        "\n",
        "Dependiendo del tipo de problema, se usa una función diferente:\n",
        "\n",
        "<b> Classificación binaria </b>\n",
        "\n",
        "* Activación de salida: `sigmoid`\n",
        "* Función de pérdida: `binary_crossentropy`\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "<b> Clasificación multiclase </b>\n",
        "\n",
        "* Activación de salida: `softmax`\n",
        "* Pérdida:`categorical_crossentropy` (si las etiquetas están codificadas en one-hot) o `sparse_categorical_crossentropy` (si las etiquetas son enteras)\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "<b> Regresión\n",
        "* Activación de salida: `linear`\n",
        "* Pérdida: `mse`(eror cuadrático medio) o `mae`(arror absoluto medio)\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',       # o 'mae'\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "1SLAugiQEY2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Optimizador </b>\n",
        "\n",
        "El optimizador define cómo se ajustan los pesos del modelo en cada iteración.\n",
        "Su función es minimizar la pérdida mediante el descenso del gradiente u otros métodos adaptativos.\n",
        "\n",
        "Los más comunes son:\n",
        "\n",
        "* `sgd`: Gradiente descendente estocástico\n",
        "```python\n",
        "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\n",
        "```\n",
        "* `adam`: Método adaptativo, rápido y estable (recomendado por defecto)\n",
        "```python\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "```\n",
        "* `rmsprop`: Ideal para redes recurrentes o datos no estacionarios\n",
        "```python\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "```"
      ],
      "metadata": {
        "id": "--D4UQucPacf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Métricas (metrics) </b>\n",
        "\n",
        "Las métricas permiten evaluar el rendimiento del modelo durante el entrenamiento y la validación. No influeyn en el proceso de optimización, solo sirven como referencia.\n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "* `accuracy`: Clasificación (binaria o multiclase): Mide el porcentaje de aciertos\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "* `mae`: Regresión: Mide el promedio del valor absoluto de los errores\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',          # pérdida puede ser MSE\n",
        "    metrics=['mae']      # seguimiento del error medio absoluto\n",
        ")\n",
        "```\n",
        "* `mse`: Regresión: Calcula el promedio de los errores al cuadrado, por lo que penaliza más los errores grandes.\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',        # pérdida igual que la métrica\n",
        "    metrics=['mse']\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "QbaqDcIlQnkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Fase de entrenamiento\n",
        "\n",
        "Una vez que el modelo ha sido construido (definición de capas y activación) y compilado (selección de la función de pérdida, el optimizador y las métricas), llega el momento clave: la fase de entrenamiento.\n",
        "\n",
        "Durante esta estapa, el modelo aprende de los datos, ajustando sus pesos internos para minimizar la función de pérdida definida en la fase de compilación."
      ],
      "metadata": {
        "id": "Xpv2JqPWSGXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Objetivo del entrenamiento </b>\n",
        "\n",
        "El objetivo principal del entrenamiento es minimizar la pérdida (`loss`). Para ello, el modelo repite un cilo de pasos en cada época (`epoch`):\n",
        "\n",
        "1. Realiza predicciones sobre los datos de entrenamienot.\n",
        "2. Calcula el error (pérdida) entre las predicciones y los valores reales,.\n",
        "3. Ajusta los pesos mediante el optimizador para reducir ese error.\n",
        "4. Evalúa el rendimiento con los datos de validación (si se han definido)"
      ],
      "metadata": {
        "id": "4FWgp8x8TkLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Método </b>`model.fit()`\n",
        "\n",
        "En keras, el entrenamiento se realiza con el método `fit()`, que ajusta el modelo a los datos de entrada.\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    x_train, y_train,      # Datos de entrenamiento\n",
        "    epochs=20,             # Número de iteraciones completas sobre los datos\n",
        "    batch_size=32,         # Tamaño de cada lote de entrenamiento\n",
        "    validation_split=0.2,  # Porcentaje de datos usados para validación\n",
        "    verbose=1              # Nivel de detalle del entrenamiento\n",
        ")\n",
        "```\n",
        "\n",
        "### <b> Parámetros principales </b>\n",
        "\n",
        "* `epochs`: número de veces que el modelo procesa todo el conjunto de entrenamiento. *Auméntalo si el modelo aún no ha aprendido lo suficiente (la pérdida sigue bajando);\n",
        "redúcelo si el modelo empieza a sobreajustar (la pérdida de validación empeora).*\n",
        "* `batch_size`: número de muestras procesadas antes de actualizar los pesos del modelo. *Auméntalo si el entrenamiento es inestable o tienes muchos datos y buena memoria;\n",
        "redúcelo si el modelo no generaliza bien o el hardware es limitado.*\n",
        "* `validation_split`: fracción del conjunto de entrenamiento reservada para validación (opcional). *Auméntalo si dispones de muchos datos y quieres una validación más fiable;\n",
        "redúcelo si el conjunto de entrenamiento es pequeño y necesitas más muestras para aprender.*\n",
        "* `validation_data`: alternativa a `validation_split`, permite especificar manualmente un conjunto `(x_val, y_val)`. *Auméntalo si dispones de muchos datos y quieres una validación más fiable;\n",
        "redúcelo si el conjunto de entrenamiento es pequeño y necesitas más muestras para aprender.*\n",
        "* `verbose`: controla la salida del proceso (0=silencio, 1=barra de progreso, 2=resumen por época). *Ponlo en 1 (barra de progreso) para entrenamientos normales y seguimiento visual;\n",
        "úsalo en 2 si solo quieres un resumen por época (entrenamientos largos o en notebooks);\n",
        "ponlo en 0 cuando no necesites mostrar información (por ejemplo, en ejecuciones automatizadas).*\n",
        "\n",
        "### <b> Objeto </b>\n",
        "\n",
        "El método `fit()` devuelve un objeto llamado `history`, que guarda la evolución de las métricas durante el entrenamiento y validación.\n",
        "\n",
        "Este objeto se utiliza posteriormente para analizar el rendimiento del modelo.\n",
        "\n",
        "```python\n",
        "print(history.history.keys())\n",
        "# Ejemplo de salida: dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
        "```"
      ],
      "metadata": {
        "id": "H9ffFIlIJ9xU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Fase de evaluación\n",
        "\n",
        "Una vez que el modelo ha sido entrenado, el siguiente paso consiste en evaluar su rendimiento. El objetivo de esta fase es comprobar cómo se comporta el modelo con daatos que no ha visto durante el entrenamiento, es decir, su capacidad de generalización."
      ],
      "metadata": {
        "id": "C-Sm7KZKUzfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Objeto de la evaluación </b>\n",
        "\n",
        "Durante el entrenamiento, el modelo mejora ajustando sus pesos para minimizar la función de péridda. Sin embargo, un modelo puede aprender demasiado bien los datos de entrenamiento ( fenómeno conocido como <b>overfitting</b>), perdiendo capacidad para generalizar.\n",
        "\n",
        "Por eso, en esta fase se evalúa el modelo sobre un conjunto de datos de prueba (test) o de validación no usados durante el entrenamiento,"
      ],
      "metadata": {
        "id": "5DQc923IMwR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b> Método `model.evaluate()`\n",
        "\n",
        "Keras proporciona el método `evaluate()`  para medir la pérdida y las métricas definidas en la compilación.\n",
        "\n",
        "```python\n",
        "results = model.evaluate(X_test,\n",
        "                        y_test, # Datos de prueba\n",
        "                        verbose=1) # Nivel de detalle de la salida\n",
        "print('Test Loss:', results)\n",
        "```"
      ],
      "metadata": {
        "id": "moPAdi67NOIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Visualización del entrenamiento </b>\n",
        "\n",
        "El objeto `history` devuelto por el `model.fit()` contiene el registro del proceso de aprendizaje. podemos usarlo para visualizar cómo evolucionaron la pérdida y la precisión a lo largo de las épocas. Aplicando la siguiente formula veremos el gráfico.\n",
        "\n",
        "```python\n",
        "def show_loss_evolution(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('MSE')\n",
        "  plt.plot(hist['epoch'],hist['loss'],label='Train Error')\n",
        "  plt.plot(hist['epoch'],hist['val_loss'],label='Val Error')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "```\n",
        "\n",
        "Interpretación:\n",
        "\n",
        "| **Patrón observado** | **Interpretación** | **Posible acción** |\n",
        "|------------------------|--------------------|--------------------|\n",
        "| Ambas bajan y se estabilizan | Aprendizaje correcto | Mantener configuración |\n",
        "| `loss ↓` pero `val_loss ↑` | Overfitting | Aplicar `Dropout`, `EarlyStopping` o regularización (`L1`/`L2`) |\n",
        "| Ambas altas y sin mejora | Underfitting | Aumentar épocas, añadir neuronas o ajustar la tasa de aprendizaje |\n",
        "| Curvas inestables | Entrenamiento irregular | Ajustar `batch_size` o normalizar los datos |"
      ],
      "metadata": {
        "id": "i-LGcMGCPfA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Fase de mejora\n",
        "\n",
        "Una vez evaluado el modelo, el siguiente paso consiste en mejorar su rendimiento y capacidad de generalización.\n",
        "Esta fase busca evitar que el modelo memorice los datos de entrenamiento (overfitting) y ajustar sus hiperparámetros para obtener la mejor configuración posible."
      ],
      "metadata": {
        "id": "VVWlMY-VQ4Ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>1.5.1 Prevención del Overfitting</b> *Regularización*\n",
        "El overfitting se produce cuando el modelo aprende demasiado los datos de entrenamiento, incluyendo su ruido, y pierde capacidad para generalizar a nuevos datos.\n",
        "\n",
        "En esta situación, el error de entrenamiento (`loss`) sigue bajando mientras el error de validación (`val_loss`) empieza a subir.\n",
        "\n",
        "Las principales técnicas aplicadas para prevenirlo son las siguientes:"
      ],
      "metadata": {
        "id": "B4nveStZRUGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a) <b>Reducción del `batch_size`</b>\n",
        "\n",
        "Reducir el tamaño del *batch* introduce más variabilidad (ruido) en las actualizaciones de los pesos, lo que mejora la capacidad de generalziación y reduce el sobreajuste.\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=16,        # Tamaño de lote reducido\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "Cuándo usarlo:\n",
        "Cuando el modelo sobreajusta y las curvas loss y val_loss se separan mucho.\n",
        "\n",
        "#### <b> b) Dropout </b>\n",
        "\n",
        "El Dropout \"apaga\" aleatoriamente un porcentaje de neuronas durante el entrenamiento. Esto impide que el modelo dependa excesivamente de conexiones específicas y mejora su capacidad de generalización.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),  # desactiva el 30% de las neuronas\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "Cuándo usarlo:\n",
        "Cuando el modelo muestra buen rendimiento en entrenamiento pero bajo en validación.\n",
        "\n",
        "#### <b>c) Batch Normalization </b>\n",
        "\n",
        "La Batch Normalization normaliza las activaciones intermedias dentro del modelo, lo que estabiliza el entrenamiento y permite usar tasas de aprendizaje más altas sin divergencias.\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),  # normaliza las activaciones\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "Cuándo usarlo:\n",
        "Cuando el entrenamiento es inestable o la pérdida oscila entre épocas.\n",
        "\n",
        "#### <b> d) Layer Normalization</b>\n",
        " La Layer Normalization normaliza las activaciones a nivel de muestra (no por batch). Es útil cuando el tamaño de batch es pequeño o variable.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    LayerNormalization(),  # normalización por muestra\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "Cuándo usarlo:\n",
        "Cuando el tamaño del batch es muy pequeño o la red presenta inestabilidad (las métricas (como la pérdida loss o la precisión accuracy) suben y bajan de forma irregular).\n",
        "\n",
        "#### <b> e) Regularización L1, L2 y Elastic Net</b>\n",
        "\n",
        "Las téncias de regularización penalizan los pesos grandes añadiendo un término a la fundión de pérdida. Esto ayuda a mantener el modelo más simple y menos propenso al overfitting.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Regularización L1\n",
        "layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001))\n",
        "\n",
        "# Regularización L2\n",
        "layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))\n",
        "\n",
        "# Regularización Elastic Net (L1 + L2)\n",
        "layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))\n",
        "```\n",
        "\n",
        "Cuándo usarlo:\n",
        "Cuando el modelo tiene muchos parámetros o hay riesgo de sobreajuste.\n",
        "\n",
        "#### <b> f) Early Stopping </b>\n",
        "\n",
        "El Early Stopping es una téncica que detiene automáticamente el entrenamiento cuando el modelo deja de mejorar el conjunto de validación. Su oobjetivo es evitar el sobreajuste y ahorrar tiempo de entrenamiento innecesario.\n",
        "\n",
        "*Funcionamiento*: Durante el entrenamiento, Keras monitoriza una métrica (por ejemplo, val_loss).\n",
        "Si esa métrica no mejora después de varias épocas consecutivas, el entrenamiento se detiene y, opcionalmente, se restauran los pesos del mejor punto alcanzado.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Definimos el callback de Early Stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',         # métrica a observar\n",
        "    patience=5,                 # nº de épocas sin mejora antes de detener\n",
        "    restore_best_weights=True,  # vuelve a los mejores pesos\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenamiento con Early Stopping\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "Parametros princiaples:\n",
        "\n",
        "* `monitor`:Métrica a vigilar (``val_loss``, ``val_accuracy``, etc.)\n",
        "* `patience`: Número de épocas que puede pasar sin mejora antes de parar\n",
        "* `restore_best_weights`: Si es ``True``, recupera automáticamente los pesos con mejor rendimiento\n",
        "* `mode`: Determina si se busca minimizar o maximizar la métrica (``min`` o ``max``)"
      ],
      "metadata": {
        "id": "v3tDTjnVryG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>1.5.2 Ajuste de Hiperparámetros (Hyperparameter Tunning) </b> *Optimización*\n",
        "Los hiperparámetros son valores que determinan cómo se entrena el modelo y no se aprenden automáticamente.\n",
        "\n",
        "Ejemplos: número de neuronas, tasa de aprendizaje, número de capas, función de activación, etc.\n",
        "\n",
        "Ajustarlos correctamente puede marcar una gran diferencia en el rendimiento del modelo.\n",
        "\n",
        "Técnicas de ajuste:"
      ],
      "metadata": {
        "id": "icsG3s-0uVcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>a) Búsqueda manual </b>\n",
        "\n",
        "Consiste en probar diferentes configuraciones y observar el rendimiento del modelo en valdiación. Es la forma más simple pero también la más lenta.\n",
        "\n",
        "Cuándo usarlo:\n",
        "Para exploraciones rápidas o cuando los recursos de cómputo son limitados.\n",
        "\n",
        "#### <b> b) Ajuste automático con Keras Tuner </b>\n",
        "\n",
        "La librería Keras Tuner permite explorar automáticamente diferentes combinaciones de hiperparámetros y encontrar la configuración óptima.\n",
        "\n",
        "Entendiendo las diferentes estrategias de tuneo:\n",
        "\n",
        "Keras Tuner nos ofrece 4 principales tecnicas de optimización de hiperparámetros:\n",
        "\n",
        "1. RandomSearch\n",
        "    * Como funciona: Selecciona aleatoriamente muestras del espacio de hiperarámetros\n",
        "    * Pros: Simple, fácilmente paralelizable, no hace suposiciones sobre la importancia de los parámetros\n",
        "    * Cons: Puede ser ineficiente en espacios de búsqueda grandes.\n",
        "    * Mejor para: exploración incial o cuando se sabe pcoo sobre el espacio de hiperparámetros.\n",
        "\n",
        "2. Hyperband\n",
        "    * Como funciona: Asigna recursos (epochs) de forma dinámica, descartando rápidamente los modelos con bajo rendimiento.\n",
        "    * Pros: Más eficiente que la búsqueda aleatoria, especialmente para redes profundas\n",
        "    * Cons: más complejo de configurar correctamente.\n",
        "    * Mejor para: Cuando el entrenamiento es computacionalmente costoso y se quiere equilibrarexploración vs explotación.\n",
        "\n",
        "3. BayesianOptimizatipon\n",
        "    * Como funciona: construe un modelo de probabilidad de la función objetivo y lo usa para seleccionar hiperparámetros\n",
        "    * Pros: uso más eiciente de los recursos, aprende de evaluaciones previas.\n",
        "    * Cons: más compleo, requiere mayor costo computacional en cada iteración\n",
        "    * Mejor para: cuando la evaluación es costosa y se tiene un espacio de búsqueda moderado.\n",
        "\n",
        "4. Sklearn\n",
        "    * Como funciona: interfaz para los métodos de búsqueda de hiperparámetros de scikit-learn.\n",
        "    * Pros: API familiar para quienes provienen de scikit-learn.\n",
        "    * Cons: limitado a las capacidades de ajuste de hiperparámetors de scikit-learn.\n",
        "    * Mejor para: cuando se integra con pipelines existentes de scikit-learn.\n",
        "\n",
        "```python\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Definición del modelo adaptable\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(10,)))\n",
        "    \n",
        "    # Número de unidades variable\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('units', min_value=32, max_value=256, step=32),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    \n",
        "    # Tasa de aprendizaje variable\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "  return model\n",
        "\n",
        "# Búsqueda aleatoria de combinaciones\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=5,\n",
        "    directory='tuning_results',\n",
        "    project_name='Regression_Tuning'\n",
        ")\n",
        "\n",
        "# Ejecución del tuning\n",
        "tuner.search(x_train, y_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "tfX0CXoivuqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6. Fase de producción\n",
        "\n"
      ],
      "metadata": {
        "id": "IjJT4V_q0zSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> 2. Convolutional Neural Network (CNN) model in Keras </b>\n",
        "\n",
        "Una Red Neuronal Convolucional (CNN) es uun tipo de red neuronal diseñada para trabajar con datos que presentan estructura espacial, como imágenes o señales. Emplea filtros (kernales) que se deslizan sobre la entrada para extraer automáticmanete características relevantes, desde patrones simples hasta representaciones más complejas. Se utiliza principalmente en tareas de clasificación y reconocimiento de imágenes, detección, segmentación y, en general visión por computador.\n",
        "\n",
        "#### <b> Estrucutra básica de una red neuronal convolucional </b> ####\n",
        "\n",
        "Una arquitectura de red neuronal convolucional consta de las siguientes partes principales:\n",
        "\n",
        "* Capa de entrada (Input Layer): recibe el tensor de los datos de entrada (por ejemplo, una imagen con dimesniones alto x ancho x canales).\n",
        "\n",
        "* Capas concolucionales (Convolutional Layers): aplican filtros que extraen características locales (bordes, texturas, formas), generando mapas de características.\n",
        "\n",
        "* Capas de reducción espacial (Pooling Layers): reducen la dimensión espacial de los mapas de características para disminuir parámetros y controalr el sobreajuste.\n",
        "\n",
        "* Capa de aplanado o agregación global (FLatten / GlobalAveragePolling): transforma los mapas de características en un vector de características o realiza un promedio global por canal.\n",
        "\n",
        "* Capas Densas (Fully connected Layers): combina las características extraídas para aprender relaciones de mayor nivel.\n",
        "\n",
        "* Capa de salida (Output layer): Produce la predicción final."
      ],
      "metadata": {
        "id": "qZgrZyL3c2iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías necesarias\n",
        "\n",
        "Antes de comenzar a construir una red neuronal, debemos importar las librerías esenciales para definir, entrenar y analizar el modelo. Estas librerías cubren distintos aspectos: modelado, preprocesamiento, visualización y optimización."
      ],
      "metadata": {
        "id": "HA4iWuskzeHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow y Keras: construcción y entrenamiento del modelo\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # para data augmentation\n",
        "\n",
        "# NumPy: operaciones matemáticas y manejo de arreglos numéricos\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib: visualización de imágenes, resultados y métricas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn: métricas adicionales de evaluación (precisión, matriz de confusión, etc.)\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "Z6Kf4ahVz0nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Fase de construcción\n",
        "\n",
        "En esta fase se define la arquitectura de la red neuronal convolucional, es decir, la secuencia de capas que la componen y la forma en la que se conectan entre sí. Cada capa tiene una función específica dentro del modelo, y juntas permiten que la red aprenda las características más relefantes de los datos.\n",
        "\n",
        "A continuación se describen las capas mas counes utilizadas en la construcción de una CNN."
      ],
      "metadata": {
        "id": "70BBrpXA0BPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 2.1.1. Capa de entrada (Input Layer) </b>\n",
        "\n",
        "(NOTA: Esta capa no utiliza función de activación)\n",
        "\n",
        "La capa de entrada define la forma de los datos que recibe el modelo. En las CNN, los datos de entrada se representan como tensores tridimensionales con las dimensiones (alto,ancho,canales).\n",
        "\n",
        "Por ejemplo, una imagen RGB de 32x32 píeles tiene forma de (32,32,3).\n",
        "\n",
        "Parámetros principales:\n",
        "\n",
        "* shaepe: dimensiones del tensor de entrada (alto,ancho,canales).\n",
        "* name: nomdre identificador de la capa (opcional).\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "inputs = layers.Input(shape=(32, 32, 3), name='input_layer')\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(32, 32, 3), name='input_layer'))\n",
        "````\n"
      ],
      "metadata": {
        "id": "37kPkv0W0Z23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 2.1.2 Capas Convolucionales (Convolutional Layers) </b>\n",
        "\n",
        "Las capas convolucionales son el núclo de una CNN.\n",
        "\n",
        "Aplican filtros (kernels) que se desplazan sobre la imagen para extraer características locals como bordes, texturas o formas.\n",
        "\n",
        "Cada filtro genera un mapa de características (feature map) que resalta la presencia de un patrín concreto.\n",
        "\n",
        "<b>Parámetros principales:</b>\n",
        "\n",
        "* `filters:` número de filtros o mapas de características.\n",
        "* `kernel_size:` tamaño del filtro (por ejemplo, (3,3)).\n",
        "* `strides:` paso de desplazamiento del filtro.\n",
        "* `padding:` ``valid`` (sin relleno) o ``same`` (mantiene el tamaño)\n",
        "* `activation:` función de activación utilizada (por ejemplo, ``relu``).\n",
        "* `name:` nombre de la capa (opcional).\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu',\n",
        "                  padding='same', name='conv_1')(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu',\n",
        "                  padding='same', name='conv_2')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1'))\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2'))\n",
        "````\n",
        "\n",
        "<b>Funciones de activación </b>\n",
        "\n",
        "* `ReLU`: la más utilizada en CNN. Ideal para clasificación de imágenes generales (por ejemplo, CIFAR-10 o MNIST).\n",
        "* `LeakyReLU`: alternativa cuando hay riesgo de que muchas neuronas queden inactivas (problema de “ReLU muerta”).\n",
        "* `ELU`: útil en modelos más profundos, mejora la convergencia manteniendo valores negativos pequeño\n",
        "* `Tanh`: poco común en CNN modernas, pero útil si los datos están centrados entre -1 y 1 (por ejemplo, imágenes normalizadas).\n"
      ],
      "metadata": {
        "id": "OL45IIHCSa6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>2.1.3 Capas de reducción espacial (Polling Layers)</b>\n",
        "(NOTA: estas capas no utilizan función de activación)\n",
        "\n",
        "Las capas de *pooling* reducen el tamaño espacial de los mapas de características, conservando la información más importante.\n",
        "\n",
        "Esto disminuye la complejidad computacional y ayuda a evitar el sobreajuste.\n",
        "\n",
        "<b>Tipos más comunes:</b>\n",
        "\n",
        "* `MaxPooling2D`: selecciona el valor máximo en cada región (más iusado en clasificación).\n",
        "* `AveragePooling2D`: calcula el promedio de los valores (útil cuando se busca suavizar ruido).\n",
        "\n",
        "<b>Parámetros princiaples:</b>\n",
        "\n",
        "* `pool_size`:= tamaño de la ventana (por ejemplo, (2,2))\n",
        "* `strides`: desplazamiento (por defecto igual a `pool_size`).\n",
        "* `padding`: 'valid' o 'same'.\n",
        "* `name`: nombre de la capa.\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "x = layers.MaxPooling2D(pool_size=(2,2), name='pool_1')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "model.add(layers.MaxPooling2D((2,2), name='pool_1'))\n",
        "````\n",
        "\n"
      ],
      "metadata": {
        "id": "63xFOdqMD1MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 2.1.4. Capa de aplanado (Flatten Layer</b>\n",
        "(NOTA: esta capa no utiliza función de activación)\n",
        "\n",
        "Convierte los mapas de característicasbidimensionales en un vector unidminensional, para conectar las capas conolucionales con las capas desas finales.\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "x = layers.Flatten(name='flatten')(x)\n",
        "````\n",
        "\n",
        "API Secuencual\n",
        "\n",
        "```python\n",
        "model.add(layers.Flatten(name='flatten'))\n",
        "````\n"
      ],
      "metadata": {
        "id": "cjUKD-t1I_we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>2.1.5. Capas densas (Fully Connected Layers) </b>\n",
        "\n",
        "las capas densas combinan las características extraídas por las capas convolucionales.\n",
        "Cada neurona se cpnecta con toas las neuronas de la capa siguiente, lo que permite al modelo aprender relaciones de alto nivel.\n",
        "\n",
        "<b>Parámetros principales</b>\n",
        "\n",
        "* `units`: número de neuronas\n",
        "* `activation`: función de activación utilizada\n",
        "* `name`: nombre de la capa\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "x = layers.Dense(units=64, activation='relu', name='dense_1')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "model.add(layers.Dense(64, activation='relu', name='dense_1'))\n",
        "````\n",
        "\n",
        "<b>Funciones de activación</b>\n",
        "\n",
        "* `ReLU`: opción general para capas ocultas intermedias. Ejemplo: clasificación de imágenes en varias clases.\n",
        "* `Tanh`: útil cuando los datos o las salidas intermedias están centrados en 0 (por ejemplo, embeddings normalizados).\n",
        "* `Sigmoid`: rara en capas ocultas, pero útil si se desea una salida intermedia entre 0 y 1 (por ejemplo, probabilidad parcial en modelos autoencoder).\n",
        "* `ELU`: recomendable en modelos más profundos donde ReLU pueda saturarse."
      ],
      "metadata": {
        "id": "GDDnT-wTJ1s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.6. Capa de salida (Ouput Layer)\n",
        "\n",
        "la capa de salida genera la predicción final del modelo.\n",
        "\n",
        "El número de neuronas y la función de activación dependen del tipo de problema que se quiera resolver.\n",
        "\n",
        "<b>Funciones de activación </b>\n",
        "\n",
        "\n",
        "| **Tipo de tarea**              | **Ejemplo**                                 | **Activación**             | **Cuándo usarla**                                                                 |\n",
        "|--------------------------------|---------------------------------------------|-----------------------------|----------------------------------------------------------------------------------|\n",
        "| **Clasificación binaria**      | Imagen con o sin gato                       | `Sigmoid`                  | Cuando hay solo dos clases posibles. Devuelve una probabilidad entre 0 y 1.     |\n",
        "| **Clasificación multiclase**   | CIFAR-10 (10 categorías)                    | `Softmax`                  | Cuando las clases son excluyentes (una sola categoría por imagen).              |\n",
        "| **Clasificación multietiqueta**| Imagen con varias etiquetas posibles        | `Sigmoid` *(una por clase)* | Cuando una imagen puede pertenecer a más de una clase (por ejemplo, “perro” y “exterior”). |\n",
        "| **Regresión**                  | Predicción de coordenadas o valores continuos| `Linear / Tanh`            | `Linear` si los valores no tienen límite; `Tanh` si están normalizados en [-1,1]. |\n",
        "| **Reconstrucción o generación**| Autoencoders o GANs                         | `Sigmoid / Tanh`           | Según el rango de valores de los píxeles de salida (`[0,1]` o `[-1,1]`).        |\n",
        "\n",
        "\n",
        "<b>Clasificación binaria (`Sigmoid`)</b>\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "layers.Dense(1, activation='sigmoid', name='output')\n",
        "````\n",
        "\n",
        "<b>Clasificación multiclase (`Softmax`)</b>\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "outputs = layers.Dense(10, activation='softmax', name='output')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "    layers.Dense(10, activation='softmax', name='output')\n",
        "````\n",
        "\n",
        "<B>Clasificación multietiqueta (`Sigmoid` por clase)</B>\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "outputs = layers.Dense(n_labels, activation='sigmoid', name='output')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "    layers.Dense(n_labels, activation='sigmoid', name='output')\n",
        "````\n",
        "<b>Regresión (``Linear`` o ``Tanh``) </b>\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "outputs = layers.Dense(1, activation='linear', name='output')(x)\n",
        "````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "    layers.Dense(1, activation='linear', name='output')\n",
        "````"
      ],
      "metadata": {
        "id": "N97hz6zpMJoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>2.1.7. Construcción completo del model (ejemplo CIFAR-10)</b>\n",
        "\n",
        "Consturcción de un modelo CIFAR-10, es decir un problema de clasificación de multiclase de imágenes.\n",
        "\n",
        "API Funcional\n",
        "\n",
        "```python\n",
        "inputs = layers.Input(shape=(32, 32, 3), name='input')\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1')(inputs)\n",
        "x = layers.MaxPooling2D((2,2), name='pool_1')(x)\n",
        "x = layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2')(x)\n",
        "x = layers.MaxPooling2D((2,2), name='pool_2')(x)\n",
        "x = layers.Flatten(name='flatten')(x)\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='output')(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs, name='CNN_model')\n",
        "model.summary()````\n",
        "\n",
        "API Secuencial\n",
        "\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(32, 32, 3), name='input'),\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same', name='conv_1'),\n",
        "    layers.MaxPooling2D((2,2), name='pool_1'),\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same', name='conv_2'),\n",
        "    layers.MaxPooling2D((2,2), name='pool_2'),\n",
        "    layers.Flatten(name='flatten'),\n",
        "    layers.Dense(64, activation='relu', name='dense_1'),\n",
        "    layers.Dense(10, activation='softmax', name='output')\n",
        "])\n",
        "\n",
        "model.summary()````"
      ],
      "metadata": {
        "id": "1zuqzqwnQCNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Fase de Compilación\n",
        "\n",
        "Una vez construida la arquitectura de la red neuronal (definidas las capas, sus funciones de activación y conexiones), el siguiente pasi es compilar el modelo.\n",
        "\n",
        "Durante la compilación, se especifica cómo el modelo va a aprender. Para ello, sedefinen tres elementos esenciales:\n",
        "\n",
        "1. La función de pérdida (loss): indica qué mide y qué debe minimizar.\n",
        "2. El optimizador (optimizier): indica cómo se actualizan los pesos para reducr la pérdida.\n",
        "3. Las métricas (metrisc). sirven para monitorizar el rendimiento durante el entrenamiento, pero no afectan al aprendizaje."
      ],
      "metadata": {
        "id": "oYmWJkwCRJAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<b> Función de péridda (loss) </b>\n",
        "\n",
        "la función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. El objetivo del entrenamiento es minimizar esta función.\n",
        "\n",
        "Dependiendo del tipo de problema, se usa una función diferente:\n",
        "\n",
        "<b>Clasificación binari </b>\n",
        "\n",
        "* Activación de salida: ``sigmoid``\n",
        "* Función de pérdida: ``binary_crossentropy``\n",
        "* ej.: “gato” vs “no gato”\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "<b>Clasificación multiclase</b>\n",
        "\n",
        "* Activación de salida: ``softmax``\n",
        "\n",
        "* Pérdida:\n",
        "    * ``categorical_crossentropy`` (si las etiquetas están codificadas en one-hot)\n",
        "    * ``sparse_categorical_crossentropy`` (si las etiquetas son enteras)\n",
        "* ej.: CIFAR-10: 10 clases exclusivas\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',  # o 'sparse_categorical_crossentropy'\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "<b>Clasificación multietiqueta</b>\n",
        "\n",
        "* Activación de salida: ``sigmoid`` (una por clase)\n",
        "\n",
        "* Función de pérdida: ``binary_crossentropy`` (aplicada de forma independiente a cada etiqueta)\n",
        "* ej.: etiquetas no excluyentes: “perro”, “exterior”, “noche”…\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "<b>Regresión</b>\n",
        "\n",
        "* Activación de salida: ``linear``\n",
        "\n",
        "* Pérdida:\n",
        "    * ``mse`` (mean squared error – error cuadrático medio)\n",
        "    * ``mae`` (mean absolute error – error absoluto medio)\n",
        "* ej.: predecir un valor continuo: brillo, edad, ángulo…\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',       # o 'mae'\n",
        "    metrics=['mae']\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "AiuHr0lSUL7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Optimizador (optimizer) </b>\n",
        "\n",
        "El optimizador define cómo se ajustan los pesos del modelo en cada interación.\n",
        "\n",
        "Su función es minimizar la pérdida mediante el descenso del gradiente o métodos adaptattivos.\n",
        "\n",
        "Las más comunes son:\n",
        "\n",
        "``Adam`` (Adaptive Moment Estimation)\n",
        "\n",
        "* Método adaptativo, rápido y estable.\n",
        "* Ajusta la tasa de aprendizaje de forma individual para cada parámetro.\n",
        "* Recomendado por defecto para la mayoría de CNN.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "```\n",
        "``RMSprop``\n",
        "\n",
        "* Variante de gradient descent adaptativa.\n",
        "* Ideal para redes con datos no estacionarios (por ejemplo, secuencias o video).\n",
        "* Controla la variabilidad de los gradientes.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "\n",
        "```\n",
        "\n",
        "``SGD`` (Stochastic Gradient Descent)\n",
        "\n",
        "* Gradiente descendente estocástico.\n",
        "* Actualiza los pesos lentamente, con posibilidad de usar momentum.\n",
        "* Ideal cuando se busca un entrenamiento controlado y estable.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\n",
        "```\n"
      ],
      "metadata": {
        "id": "gGXgnQ9rYU-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>Métricas (metrics)</b>\n",
        "\n",
        "Las métricas permiten evaluar el rendimeinto del modelo durante el entrenamiento y la validación. No influyen en el proceso de optimización, pero sirven como referencia para analizar el desempeño.\n",
        "\n",
        "``Accuracy``\n",
        "* Tipo de problema: Clasificación binaria o multiclase.\n",
        "* Descripción: Mide el porcentaje de aciertos sobre el total de predicciones.\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "``mae (Mean Absolute Error)``\n",
        "* Tipo de problema: Regresión.\n",
        "* Descripción: Calcula el promedio del valor absoluto de los errores.\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',      # pérdida puede ser MSE\n",
        "    metrics=['mae']  # seguimiento del error medio absoluto\n",
        ")\n",
        "```\n",
        "\n",
        "``mse (Mean Squared Error)``\n",
        "* Tipo de problema: Regresión.\n",
        "* Descripción: Calcula el promedio de los errores al cuadrado (penaliza más los errores grandes)\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',      # pérdida igual a la métrica\n",
        "    metrics=['mse']\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "E600SqL1doXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Fase de entrenamiento\n",
        "\n",
        "Una vez compilado el modelo, se procede a entrenarlo.\n",
        "\n",
        "Durante esta fase, el modelo ajusta sus pesos internos para minimizar la función de péridad definda en la compilación y mejorar las métricas.\n",
        "\n",
        "El enternamiento se realiza con el método `fit()`, que recibe los atos, el número de épocas, el tamaño de los lotes y, opcionalmente, un conjunto de validación.\n"
      ],
      "metadata": {
        "id": "ibWa77yQ5f8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<b> Parámetros del método </b>\n",
        "\n",
        "* ``x, y``: datos de entrada y etiquetas verdaderas.\n",
        "\n",
        "* ``epochs``: número de pasadas completas sobre el conjunto de entrenamiento.\n",
        "\n",
        "* ``batch_size``: tamaño del lote (muestras procesadas antes de actualizar pesos).\n",
        "\n",
        "* ``validation_data``: tupla (x_val, y_val) para evaluar al final de cada época.\n",
        "\n",
        "* ``validation_split``: fracción de x/y usada automáticamente como validación (si no se pasa validation_data).\n",
        "\n",
        "* ``verbose``: nivel de detalle (0: silencioso, 1: barra de progreso, 2: por época).\n",
        "\n",
        "<b> Entrenamiento (clasificación – CNN con CIFAR-10) </b>\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    x_train, y_train,                 # Datos de entrenamiento\n",
        "    epochs=20,                        # Número de épocas\n",
        "    batch_size=64,                    # Tamaño del lote\n",
        "    validation_data=(x_val, y_val),   # Conjunto de validación\n",
        "    verbose=1                         # Mostrar progreso\n",
        ")\n",
        "```\n",
        "\n",
        "* uso típico con saldia `softmax` y pérdida `categorical_crossentropy`/`spare_categorical_crossentropy`\n",
        "* `history` almacena la evolución de loss y métricas por época (incluyendo las de validación si se proporcionan).\n",
        "\n",
        "\n",
        "<b> Entrenamiento (regresión – predicción continua) </b>\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,  # Reserva automáticamente el 20% para validación\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* Uso típico con salida `linear` y pérdidas de regresión (`mse` o `mae`).\n",
        "\n",
        "* `validation_split` es útil cuando no se dispone de un conjunto de validación separado."
      ],
      "metadata": {
        "id": "IOMieFvjCbbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Fase de Evaluación\n",
        "\n",
        "Tras el entrenamiento, se evalúa el modelo para medir su rendimiento en datos no vistos y analizar su comportamiento (aciertos/errores).\n",
        "\n",
        "La evaluación se realioza con:\n",
        "\n",
        "* `model.evaluate()`: calcula la pérdida y las métricas en el conjunto de test.\n",
        "* `model.predict()`: obtiene las probabilidades/logits para analizar predicciones (por ejemplo, clases previstas).\n",
        "\n",
        "* Gráficas de evolución (a partir de `history`): ayudan a revisar cómo ha aprendido el modelo,\n",
        "* Inspección de errores: visualziar ejemplos mal clasificados con sus probabilidades.\n",
        "\n"
      ],
      "metadata": {
        "id": "LK0CJ0B2-F63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<b> Evolución cuantitativa con `evaluate()` </b>\n",
        "\n",
        "```python\n",
        "# Evalúa en el conjunto de test (p. ej., CIFAR-10)\n",
        "results = model.evaluate(X_test_norm, y_test, verbose=1)\n",
        "```\n",
        "* results[0] → pérdida en test.\n",
        "\n",
        "* results[1] → métrica principal (por ejemplo, accuracy).\n",
        "\n",
        "<b> Prediccions con `predict()` y clases previstas </b>\n",
        "\n",
        "```python\n",
        "# Probabilidades/logits para cada imagen\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Clases previstas (si la salida es softmax/multiclase)\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "```\n",
        "* Útil para analziar casos concretos, construir informes o generar visualizaciones\n",
        "\n",
        "\n",
        "<b> Gráficas de entrenamiento (pérdida y precisión) </b>\n",
        "\n",
        "Estas gráficas se generan a partir del objeto `history` devuelto por `model.fit()`.\n",
        "Sirven para evaluar el proceso de aprendizaje (sesgo/varianza, sobreajuste, etc.).\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "def show_loss_accuracy_evolution(history):\n",
        "    # Convierte el historial a DataFrame y añade el índice de época\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    # Dos subgráficos: pérdida y precisión\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Pérdida\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.plot(hist['epoch'], hist['loss'], label='Train Loss')\n",
        "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "\n",
        "    # Precisión (si está disponible en history)\n",
        "    if 'accuracy' in hist and 'val_accuracy' in hist:\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.plot(hist['epoch'], hist['accuracy'], label='Train Acc')\n",
        "        ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Acc')\n",
        "        ax2.grid()\n",
        "        ax2.legend()\n",
        "    plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "Lectura rápida de las curvas:\n",
        "\n",
        "* Paralelas y descendentes (loss) con val_loss similar → buen aprendizaje.\n",
        "* Train loss ↓ pero val_loss ↑ → posible sobreajuste (overfitting).\n",
        "\n",
        "\n",
        "<b> Inspección cualitativa de errores </b>\n",
        "\n",
        "Visualiza ejemplos del conjunto de validación mal clasificados, mostrando la clase predicha, la clase real y sus probabilidades.\n",
        "(Requiere un `val_ds` de tipo `tf.data.Dataset` y `class_names_list` con los nombres de las clases).\n",
        "\n",
        "```python\n",
        "def show_errors(val_ds, model, class_names_list, n_images=10):\n",
        "    n_plots = 0\n",
        "    for images, labels in val_ds:\n",
        "        # Predicciones para este batch\n",
        "        pred_probs = model.predict(images)\n",
        "        pred_classes = np.argmax(pred_probs, axis=-1)\n",
        "\n",
        "        # Compara con etiquetas reales\n",
        "        for ind in range(len(images)):\n",
        "            if n_plots >= n_images:\n",
        "                return\n",
        "\n",
        "            real_idx = labels[ind].numpy()\n",
        "            pred_idx = pred_classes[ind]\n",
        "\n",
        "            # Si hay error, lo mostramos\n",
        "            if pred_idx != real_idx:\n",
        "                pred_class = class_names_list[pred_idx]\n",
        "                real_class = class_names_list[real_idx]\n",
        "\n",
        "                prob_pred = float(np.max(pred_probs[ind]))\n",
        "                prob_real = float(pred_probs[ind][real_idx])\n",
        "\n",
        "                plt.imshow(images[ind].numpy().astype(\"uint8\"))\n",
        "                plt.title(\n",
        "                    f\"Predicted: {pred_class}, prob: {prob_pred:.2f}\\n\"\n",
        "                    f\"Real: {real_class}, prob: {prob_real:.2f}\"\n",
        "                )\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                n_plots += 1\n",
        "\n",
        "```\n",
        "\n",
        "Utilidad:\n",
        "\n",
        "* Detectar patrones de error (clases confundidas, fondos complejos, iluminación, etc.).\n",
        "\n",
        "* Guiar decisiones en la Fase de Mejora (data augmentation específico, arquitectura, regularización)"
      ],
      "metadata": {
        "id": "Zkq81VoyCg5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Fase de mejora\n",
        "\n",
        "Una vez evaluado el modelo de red neuronal convolucional (CNN), el siguiente paso consiste en optimizar su rendimiento y capacidad de generalización.\n",
        "\n",
        "Esta fase busca reducir el sobreajuste (overfitting), mejorar la precisión en los datos de validación y ajusta la arquitectura y los hiperparámetros de las capas convolucionales para obtener el mejor desempeño posible.\n"
      ],
      "metadata": {
        "id": "3rnB6r2OCNdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 2.5.1. Prevención del Overfitting</b> *Regularización*\n",
        "\n",
        "El overfitting ocurre cuando la CNN aprende demsiado los detalles y el ruido de las imágenes de entrenamiento, perdiendo capacidad de generalizar nuevas imagenes.\n",
        "\n",
        "En este caso, la loss de entrenamiento continúa disminuyendo mientras la val_loss comienza a aumentar.\n",
        "\n",
        "Las principales técnicas aplicadas para prevenirlo son las siguientes:\n",
        "\n"
      ],
      "metadata": {
        "id": "BwMC8MBQWCyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>a) Data Augmentation (Aumento de datos) </b>\n",
        "\n",
        "El Data Augmentation genera nuevas imágenes a partir de las originales mediante transformaciones aleatorias (rotaciones, giros, zoom, traslaciones, etc.).\n",
        "Esto amplía el conjunto de entrenamiento sin necesidad de recopilar más datos reales, lo que mejora la generalización del modelo.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)\n",
        "```\n",
        "\n",
        "Cuándo usarlo: Cuando el conjunto de imágenes es pequeño o el modelo sobreajusta rápidamente (alta precisión en entrenamiento pero baja en validación).\n",
        "\n",
        "\n",
        "<b> b) Dropout en capas convolucionales </b>\n",
        "\n",
        "El Dropout desactiva aleatoriamente un porcentaje de neuronas durante el entrenamiento, lo que evita la dependencia excesiva de ciertas activaciones y mejora la robustez del modelo.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),  # desactiva el 25% de las neuronas\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25), # desactiva el 25% de las neuronas\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5), # desactiva el 25% de las neuronas\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "Cuándo usarlo: Cuando el modelo muestra alta precisión en el entrenamiento y baja precisión en validación.\n",
        "\n",
        "<b> c) Batch Normalization </b>\n",
        "\n",
        "La Batch Normalization normaliza las activaciones intermedias en cada mini-lote, estabilizando el entrenamiento y acelerando la convergencia.\n",
        "En las CNN, suele colocarse después de las capas convolucionales y antes de la función de activación.\n",
        "\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "Cuándo usarlo: Cuando el entrenamiento es inestable, la pérdida oscila entre épocas o la red tarda en converger.\n",
        "\n",
        "<b> d) Early Stopping </b>\n",
        "\n",
        "El Early Stopping detiene automáticamente el entrenamiento cuando el rendimiento en validación deja de mejorar, evitando el sobreentrenamiento y reduciendo el tiempo total de entrenamiento.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "```\n",
        "\n",
        "Parámetros principales:\n",
        "\n",
        "* `monitor`: métrica a observar (por ejemplo, val_loss o val_accuracy).\n",
        "\n",
        "* `patience`: número de épocas sin mejora antes de detener.\n",
        "\n",
        "* `restore_best_weights`: recupera los mejores pesos previos al sobreajuste.\n",
        "\n",
        "Cuándo usarlo: Siempre que el entrenamiento prolongado empiece a degradar el rendimiento de validación.\n",
        "\n",
        "<b> e) Regularización L2 en capas convolucionales </b>\n",
        "\n",
        "La regularización L2 penaliza los pesos grandes, fomentando una distribución más equilibrada de las activaciones.\n",
        "En CNN se aplica frecuentemente a las capas convolucionales y densas.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, (3,3), activation='relu',\n",
        "                  kernel_regularizer=regularizers.l2(0.001),\n",
        "                  input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "Cuándo usarlo: Cuando la red tiene muchas capas o parámetros y se observa sobreajuste."
      ],
      "metadata": {
        "id": "tG4qmst0ZaRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5.2. Ajuste de Hiperparámetros (Hyperparameter Tunning)\n",
        "\n",
        "Los hiperparámetros en una CNN definen la estructura y el comportamiento de la red, y su correcta selección puede marcar una gran diferencia en el rendimiento final.\n",
        "\n",
        "Ejemplos comunes:\n",
        "\n",
        "* Tamaño del kernel (3×3, 5×5)\n",
        "\n",
        "* Número de filtros por capa\n",
        "\n",
        "* Tipo de pooling (Max o Average)\n",
        "\n",
        "* Tasa de Dropout\n",
        "\n",
        "* Tasa de aprendizaje (learning rate)"
      ],
      "metadata": {
        "id": "ia0226Ubb33y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Fase de producción"
      ],
      "metadata": {
        "id": "IVxiKFHJ1GEm"
      }
    }
  ]
}